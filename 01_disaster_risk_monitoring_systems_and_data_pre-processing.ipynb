{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f36efa-b9f8-4935-a403-a4010cdbad8e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ddcdef-b9e4-4ce6-8171-a9ff2a54417f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disaster Risk Monitoring Using Satellite Imagery #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf289f-fb0b-46be-8eb5-b2b5ac55d419",
   "metadata": {},
   "source": [
    "## 01 - Disaster Risk Monitoring Systems and Data Pre-processing ##\n",
    "In this notebook, you will learn the motivation behind this disaster risk monitoring and how to use hardware accelerated tools to process large image data. \n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Disaster Risk Monitoring](#s1-1)\n",
    "    * [Flood Detection](#s1-1.1) \n",
    "    * [Satellite Imagery](#s1-1.2)\n",
    "    * [Computer Vision](#s1-1.3)\n",
    "    * [Deep Learning-Based Disaster Risk Monitoring Systems](#s1-1.4)\n",
    "2. [Deep Learning Model Training Workflow](#s1-2)\n",
    "    * [Deep Learning Challenges](#s1-2.1)\n",
    "3. [Introducing the Data Set](#s1-3)\n",
    "    * [Sentinel-1 Data Public Access](#s1-3.1)\n",
    "    * [Exploratory Data Analysis](#s1-3.2)\n",
    "    * [Exercise #1 - Count Input Data](#s1-e1)\n",
    "    * [Exercise #2 - Explore Tiles](#s1-e2)\n",
    "4. [Data Pre-processing With DALI](#s1-4)\n",
    "    * [DALI Pipeline](#s1-4.1)\n",
    "    * [Data Augmentation](#s1-4.2)\n",
    "    * [Exercise #3 - Data Augmentation on Batch](#s1-e3)\n",
    "    * [Random Rotation](#s1-4.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6fc41-2ebf-43ea-a76f-c5e33780246b",
   "metadata": {},
   "source": [
    "<a name='s1-1'></a>\n",
    "## Disaster Risk Monitoring ##\n",
    "Natural disasters such as flood, wildfire, drought, and severe storms wreak havoc throughout the world, causing billions of dollars in damages, and uprooting communities, ecosystems, and economies. The ability to detect, quantify, and potentially forecast natural disasters can help us minimize their adverse impacts on the economy and human lives. While this lab focuses primarily on detecting flood events, it should be noted that similar applications can be created for other natural disasters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259981a-4bd0-42b1-9881-eda83d55c693",
   "metadata": {},
   "source": [
    "<a name='s1-1.1'></a>\n",
    "### Flood Detection ###\n",
    "A [Flood](https://en.wikipedia.org/wiki/Flood) is an overflow of water that submerges land that is usually dry. They can occur under several conditions: \n",
    "* Overflow of water from water bodies, in which the water overtops or breaks levees (natural or man-made), resulting in some of that water escaping its usual boundaries\n",
    "* Accumulation of rainwater on saturated ground in an areal flood\n",
    "* When flow rate exceeds the capacity of the river channel\n",
    "\n",
    "Unfortunately, flooding events are on the rise due to climate change and sea level rise. Due to the increase in frequency and intensity, the topic of flood has garnered international attention in the past few years. In fact, organizations such as the United Nations has maintained effective response and proactive risk assessment for flood in their [Sustainable Development Goals](https://en.wikipedia.org/wiki/Sustainable_Development_Goals). The research of flood events and their evolution is an interdisciplinary study that requires data from a variety of sources such as: \n",
    "* Live Earth observation data via satellites and surface reflectance\n",
    "* Precipitation, runoff, soil moisture, snow cover, and snow water equivalent\n",
    "* Topography and meteorology\n",
    "\n",
    "The ability to detect flood and measure the extent of the disaster, can help decision makers develop tactical responses and scientists study flood behavior over time. Ultimately, we want to enable long-term mitigation strategies that are informed by science to help us achieve sustainability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc4f55-fbaa-473a-a11c-c7bf25085db9",
   "metadata": {},
   "source": [
    "<a name='s1-1.2'></a>\n",
    "### Satellite Imagery ###\n",
    "In this lab, we demonstrate the ability to create a flood detection segmentation model using satellite imagery. Using satellites to study flood is advantageous since physical access to flooded areas is limited and deploying instruments in potential flood zones can be dangerous. Furthermore, satellite remote sensing is much more efficient than manual or human-in-the-loop solutions. \n",
    "\n",
    "There are thousands of man-made satellites currently active in space. Once launched, a satellite is often placed in one of several orbits around Earth, depending on what the satellite is designed to achieve. Some satellites, such as those discussed in this lab, are used for Earth observation to help scientists learn about our planet while others could be used for communication or navigation purposes. \n",
    "<p><img src='images/orbits.png' width=720></p>\n",
    "\n",
    "Earth observation satellites have different capabilities that are suited for their unique purposes. To obtain detailed and valuable information for flood monitoring, satellite missions such as [Copernicus Sentinel-1](https://sentinel.esa.int/web/sentinel/missions/sentinel-1), provides C-band [**Synthetic Aperture Radar**](https://en.wikipedia.org/wiki/Synthetic-aperture_radar) (SAR) data. Satellites that use SAR, as oppose to _optical_ satellites that use visible or near-infrared bands, can operate day and night as well as under cloud cover. This form of radar is used to create two-dimensional images or three-dimensional reconstructions of objects, such as landscape. The two polar-orbiting Sentinel-1 satellites (Sentinel-1A and Sentinel-1B) maintain a repeat cycle of just _6_ days in the Lower Earth Orbit (LEO). Satellites that orbit close to Earth in the LEO enjoy the benefits of faster orbit speed and data transfer. These features make the Sentinel-1 mission very useful for monitoring flood risk over time. Thus, an real-time AI-based remote flood level estimation via Sentinel-1 data can prove game-changing. \n",
    "<p><img src='images/sentinel-1.jpg' width=720></p>\n",
    "\n",
    "More information about the Sentinel-1 mission can be found [here](https://directory.eoportal.org/web/eoportal/satellite-missions/c-missions/copernicus-sentinel-1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e736e4-3225-49c2-84d3-d112adfe5e8d",
   "metadata": {},
   "source": [
    "<a name='s1-1.3'></a>\n",
    "### Computer Vision ###\n",
    "At the heart of this type of disaster risk monitoring system is one or more machine learning models to generate insights from input data. These are generally deep learning neural network models that have been trained for a specific task. There are numerous approaches for drawing insight from images using machine learning such as: \n",
    "* **Classification** is used for identifying the object contained in an image. It is the task of labeling the given frame with one of the classes that the model has been trained with \n",
    "* **Object detection**, which includes image localization, can specify the location of multiple objects in a frame\n",
    "    * **Localization** uses regression to return the coordinates of the potential object within the frame\n",
    "* **Segmentation** provides pixel level accuracy by creating a fine-grained segmentation mask around the detected object. Applications for segmentation include: an AI powered green screen to blur or change the background of the frame, autonomous driving where you want to segment the road and background, or for manufacturing to identify microscopic level defects. \n",
    "    * **Semantic segmentation** associates every pixel of an image with a class label such as flood and not-flood. It treats multiple objects of the same class as a single entity. \n",
    "    * In contrast, **instance segmentation** treats multiple objects of the same class as distinct individual instances. \n",
    "\n",
    "<p><img src='images/computer_vision_tasks.jpg' width=720></p>\n",
    "\n",
    "For the purposes of detecting flood events, we will develop a _semantic segmentation_ model trained with labelled images that are generated from Sentinel-1 data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb828f81-e0da-4bb7-80bd-106289696723",
   "metadata": {},
   "source": [
    "<a name='s1-1.4'></a>\n",
    "### Deep Learning-Based Disaster Risk Monitoring System ###\n",
    "The system that we envision consists of the below workflow: \n",
    "1. Satellite remote sensing to captures data\n",
    "2. Data is used to (continuously) train deep learning neural network models\n",
    "3. Different models and versions are managed by the model repository\n",
    "4. Model inference performance is actively monitored\n",
    "5. Data is also passed to the inference server to generate insight\n",
    "6. The deep learning-based insight can be used further analysis and/or raise alerts\n",
    "<p><img src='images/system_1.png' width=720></p>\n",
    "\n",
    "When processing data in real-time, this system can help us in delineating open water flood areas. In addition, identifying flood levels will enable effective disaster response and mitigation. If we combine the flood extent mapping with local topography, we can create a plan of action with downstream. We can use this information to predict the direction of flow of water, redirect flood waters, organize resources for distribution, etc. Such a \n",
    "system can also recommend a path of least flood levels in real-time that disaster response professionals can potentially adopt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35100f00-3a87-4e74-a800-da45cd276f4e",
   "metadata": {},
   "source": [
    "<a name='s1-2'></a>\n",
    "## Deep Learning Model Training Workflow ## \n",
    "Building a deep learning model consists of several steps, including collecting large, high-quality data sets, preparing the data, training the model, and optimizing the model for deployment. When we train a neural network model with supervised learning, we leverage its ability to perform automatic feature extraction from raw data and associate them to our target. Generally, deep learning model performance increases when we train with more data, but the process is time consuming and computationally intensive. Once a model is trained, it can be deployed and used for inference. The model can be further fine-tuned and optimized to deliver the right level of accuracy and performance. \n",
    "<p><img src='images/model_training_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25feff-85c3-483f-8d4f-158a8b8f890d",
   "metadata": {},
   "source": [
    "<a name='s1-2.1'></a>\n",
    "### Deep Learning Challenges ###\n",
    "There are some common challenges related to developing deep learning-based solutions: \n",
    "* Training accurate deep learning models from scratch requires a large amount of data and acquiring them is a costly process since they need to be annotated, often manually\n",
    "* Development often requires knowledge of one or more deep learning frameworks, such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), or [Caffe](https://caffe.berkeleyvision.org/)\n",
    "* Deep learning models require significant effort to fine-tune before it is optimized for inference and production ready\n",
    "* Processing data in real-time is computationally intensive and needs to be facilitated by software and hardware that enables low latency and high throughput\n",
    "\n",
    "As we will demonstrate, NVIDIA's [DALI](https://developer.nvidia.com/dali), [TAO Toolkit](https://developer.nvidia.com/tao), [TensorRT](https://developer.nvidia.com/tensorrt), and [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server), can be used to tackle these challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3238f-e93f-466d-8148-fe9f548f1dd4",
   "metadata": {},
   "source": [
    "<a name='s1-3'></a>\n",
    "## Introducing the Data Set ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffbac6-ac2b-4120-b941-e4e460e1a930",
   "metadata": {},
   "source": [
    "<a name='s1-3.1'></a>\n",
    "### Sentinel-1 Data Public Access ###\n",
    "The Sentinel-1 SAR data we will use is available from [ESA](https://www.esa.int/) via the [Copernicus Open Access Hub](https://scihub.copernicus.eu/). They maintain an archive and is committed to delivering data within 24 hours of acquisition and maintains recent months of data. They are also available via NASA's [EARTHDATASEARCH](https://search.earthdata.nasa.gov/) or [Vertex](https://search.asf.alaska.edu/#/), Alaska Satellite Facility's data portal. They are organized as tiles, which is the process of subdividing geographic data into pre-defined roughly-squares. Tile-based mapping efficiently renders, stores, and retrieves image data. \n",
    "<p><img src='images/sentinel-1_capture.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023e5b2-6e54-4630-8101-be70eb606f88",
   "metadata": {},
   "source": [
    "<a name='s1-3.2'></a>\n",
    "### Exploratory Data Analysis ###\n",
    "We have organized the `data` directory as below. For each image tile, there are three pieces of information: `image`, `mask`, and `catalog.json`. The catalog describes the meta data associated with the image such as the geographic coordinates boundaries (longitude, latitude) and timestamp. In addition, there is a `Sen1Floods11_Metadata.geojson` file that records the manifest by a specific region at a specific time. \n",
    "\n",
    "```\n",
    "root@server:/data$ tree\n",
    ".\n",
    "├── catalog\n",
    "│   └── sen1floods11_hand_labeled_source\n",
    "│       ├── region_1\n",
    "│       │   └── region_1.json\n",
    "│       ├── region_2\n",
    "│       │   └── region_2.json\n",
    "│\n",
    "├── images\n",
    "│   └── all_images\n",
    "│       ├── region_1.png\n",
    "│       ├── region_2.png\n",
    "│\n",
    "├── masks\n",
    "│   └── all_masks\n",
    "│       ├── region_1.png\n",
    "│       ├── region_2.png\n",
    "│\n",
    "└── Sen1Floods11_metadata.geojson\n",
    "```\n",
    "<p><img src='images/input_and_mask.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ded5ce-e5be-41fa-ba4c-af068766a6bb",
   "metadata": {},
   "source": [
    "First, we need to unzip the `flood_data.zip` file, which includes all our data. We can also count the number of images and masks to ensure they match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d10c0e4-f2af-4935-aa5c-fb0ee1193197",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# unzip data file\n",
    "!unzip data/flood_data.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4131f10-c1b7-48e6-83fa-297b29e06bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# set environment variables\n",
    "%set_env LOCAL_DATA_DIR=/dli/task/data\n",
    "\n",
    "# set paths for images and masks\n",
    "image_dir=os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images')\n",
    "mask_dir=os.path.join(os.getenv('LOCAL_DATA_DIR'), 'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5511360-f622-4345-a071-098b99981499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define function to count number of images per region\n",
    "def count_num_images(file_dir): \n",
    "    \"\"\"\n",
    "    This function returns a dictionary representing the count of images for each region as the key. \n",
    "    \"\"\"\n",
    "    # list all files in the directory\n",
    "    file_list=os.listdir(file_dir)\n",
    "    region_count={}\n",
    "    # iterate through the file_list and count by region\n",
    "    for file_name in file_list: \n",
    "        region=file_name.split('_')[0]\n",
    "        if (len(file_name.split('.'))==2) and (region in region_count): \n",
    "            region_count[region]+=1\n",
    "        elif len(file_name.split('.'))==2: \n",
    "            region_count[region]=1\n",
    "    return region_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83848268-6fb7-4d85-9379-5579e9311cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# count images and masks by region\n",
    "images_count=count_num_images(os.path.join(image_dir, 'all_images'))\n",
    "masks_count=count_num_images(os.path.join(mask_dir, 'all_masks'))\n",
    "\n",
    "# display counts\n",
    "print(f'-----number of images: {sum(images_count.values())}-----')\n",
    "display(sorted(images_count.items(), key=lambda x: x[1]))\n",
    "\n",
    "print(f'-----number of masks: {sum(masks_count.values())}-----')\n",
    "display(sorted(masks_count.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de838-d48d-4c1c-a443-9e80d7543197",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1-e1'></a>\n",
    "### Exercise #1 - Count Input Data ###\n",
    "Let's count the number of images in our training data set. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>` only to the number of tiles in the data set. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7ad9360-0393-43d9-817e-32c87faeb18a",
   "metadata": {},
   "source": [
    "The total number of tiles is: <<<<FIXME>>>>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6dbe5142-e5cb-4160-9ee6-9c14653d8e33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "The total number of tiles is: 446"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02ee7c-09eb-4052-950c-a1fd10a5a3bc",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958d428-9a06-423f-ac79-9ff573d236b3",
   "metadata": {},
   "source": [
    "Each image and mask are accompanied by a JSON file that documents its latitude and longitude. We can go through the manifest to get a sense of how our data is populated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38673725-d9d7-4844-a0dc-f210774112d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define function to get coordinates from catalog\n",
    "def get_coordinates(catalog_dir): \n",
    "    \"\"\"\n",
    "    This function returns a list of boundaries for every image as [[lon, lat], [lon, lat], [lon, lat], etc.] in the catalog. \n",
    "    \"\"\"\n",
    "    catalog_list=os.listdir(catalog_dir)\n",
    "    all_coordinates=[]\n",
    "    for catalog in catalog_list: \n",
    "        # check if it's a directory based on if file_name has an extension\n",
    "        if len(catalog.split('.'))==1:\n",
    "            catalog_path=f'{catalog_dir}/{catalog}/{catalog}.json'\n",
    "            # read catalog\n",
    "            with open(catalog_path) as f: \n",
    "                catalog_json=json.load(f)\n",
    "            # parse out coordinates\n",
    "            coordinates_list=catalog_json['geometry']['coordinates'][0]\n",
    "            lon=[coordinates[0] for coordinates in coordinates_list]\n",
    "            all_coordinates.append(lon)\n",
    "            lat=[coordinates[1] for coordinates in coordinates_list]\n",
    "            all_coordinates.append(lat)\n",
    "    return all_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa85ef-961e-430b-8361-00691d8f668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# set paths for images catalog\n",
    "image_catalog_dir=os.path.join(os.getenv('LOCAL_DATA_DIR'), 'catalog', 'sen1floods11_hand_labeled_source')\n",
    "image_coordinates_list=get_coordinates(image_catalog_dir)\n",
    "\n",
    "# flatten lat and lon coordinate lists\n",
    "image_lon=[image_coordinates_list[x] for x in range(len(image_coordinates_list)) if x%2==0]\n",
    "image_lon=np.concatenate(image_lon).ravel()\n",
    "image_lat=[image_coordinates_list[x] for x in range(len(image_coordinates_list)) if x%2==1]\n",
    "image_lat=np.concatenate(image_lat).ravel()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.scatter(image_lon, image_lat)\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Training Data Distribution')\n",
    "plt.xlim([-100, 110])\n",
    "plt.ylim([-30, 45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4068e-30e6-401c-b6e2-04c4da0e6117",
   "metadata": {},
   "source": [
    "<p><img src='images/check.png' width=720></p>\n",
    "Out of the entire Earth, we only have a small number of tiles available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a844e6e-ee9a-441b-bbdd-c6e5ae971b59",
   "metadata": {},
   "source": [
    "Let's now take a look at the specific areas. We have provided three helper functions `get_extent(file_path)`,  `tiles_by_country(country_name, image_type)` and `tiles_by_bound(top_left, bottom_right, image_type)`, where `image_type` is either `masks` or `images`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2bd988-79f5-46f1-9e61-ff44e51a2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define function to get extent of an image from catalog\n",
    "def get_extent(file_path): \n",
    "    \"\"\"\n",
    "    This function returns the extent as [left, right, bottom, top] for a given image. \n",
    "    \"\"\"\n",
    "    # read catalog for image\n",
    "    with open(file_path) as f: \n",
    "        catalog_json=json.load(f)\n",
    "    coordinates=catalog_json['geometry']['coordinates'][0]\n",
    "    coordinates=np.array(coordinates)\n",
    "    # get boundaries\n",
    "    left=np.min(coordinates[:, 0])\n",
    "    right=np.max(coordinates[:, 0])\n",
    "    bottom=np.min(coordinates[:, 1])\n",
    "    top=np.max(coordinates[:, 1])\n",
    "    return left, right, bottom, top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae37a1-df53-4499-9e25-8357a2f80528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define function to plot by region\n",
    "def tiles_by_region(country_name, plot_type='images'): \n",
    "    # set catalog and images/masks path\n",
    "    catalog_dir=os.path.join(os.getenv('LOCAL_DATA_DIR'), 'catalog', 'sen1floods11_hand_labeled_source')\n",
    "    if plot_type=='images': \n",
    "        dir=os.path.join(image_dir, 'all_images')\n",
    "        cmap='viridis'\n",
    "    elif plot_type=='masks': \n",
    "        dir=os.path.join(mask_dir, 'all_masks')\n",
    "        cmap='gray'\n",
    "    else: \n",
    "        raise Exception('Bad Plot Type')\n",
    "\n",
    "    # initiate figure boundaries, which will be modified based on the extent of the tiles\n",
    "    x_min, x_max, y_min, y_max=181, -181, 91, -91\n",
    "    fig=plt.figure(figsize=(15, 15))\n",
    "    ax=plt.subplot(111)\n",
    "    \n",
    "    # iterate through each image/mask and plot\n",
    "    file_list=os.listdir(dir)\n",
    "    for each_file in file_list:\n",
    "        # check if image/mask is related to region and a .png file\n",
    "        if (each_file.split('.')[-1]=='png') & (each_file.split('_')[0]==country_name): \n",
    "            # get boundaries of the image\n",
    "            extent=get_extent(f\"{catalog_dir}/{each_file.split('.')[0]}/{each_file.split('.')[0]}.json\")\n",
    "            x_min, x_max=min(extent[0], x_min), max(extent[1], x_max)\n",
    "            y_min, y_max=min(extent[2], y_min), max(extent[3], y_max)\n",
    "            image=mpimg.imread(f'{dir}/{each_file}')\n",
    "            plt.imshow(image, extent=extent, cmap=cmap)\n",
    "    \n",
    "    # set boundaries of the axis\n",
    "    ax.set_xlim([x_min, x_max])\n",
    "    ax.set_ylim([y_min, y_max])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b89846-5ad7-4440-8bbe-22f562837e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define function to plot by boundaries\n",
    "def tiles_by_boundaries(top_left, bottom_right, plot_type='images'): \n",
    "    # set catalog and images/masks path\n",
    "    catalog_dir=os.path.join(os.getenv('LOCAL_DATA_DIR'), 'catalog', 'sen1floods11_hand_labeled_source')\n",
    "    if plot_type=='images': \n",
    "        dir=os.path.join(image_dir, 'all_images')\n",
    "        cmap='viridis'\n",
    "    elif plot_type=='masks': \n",
    "        dir=os.path.join(mask_dir, 'all_masks')\n",
    "        cmap='gray'\n",
    "    else: \n",
    "        raise Exception('Bad Plot Type')\n",
    "\n",
    "    # initiate figure boundaries, which will be modified based on the extent of the tiles\n",
    "    x_min, x_max, y_min, y_max=top_left[0], bottom_right[0], bottom_right[1], top_left[1]\n",
    "    ax_x_min, ax_x_max, ax_y_min, ax_y_max=181, -181, 91, -91\n",
    "\n",
    "    fig=plt.figure(figsize=(15, 15))\n",
    "    ax=plt.subplot(111)\n",
    "\n",
    "    # iterate through each image/mask and plot\n",
    "    file_list=os.listdir(dir)\n",
    "    for each_file in file_list: \n",
    "        # check if image/mask is a .png file\n",
    "        if each_file.split('.')[-1]=='png': \n",
    "            # get boundaries of the image/mask\n",
    "            extent=get_extent(f\"{catalog_dir}/{each_file.split('.')[0]}/{each_file.split('.')[0]}.json\")\n",
    "            (left, right, bottom, top)=extent\n",
    "            if (left>x_min) & (right<x_max) & (bottom>y_min) & (top<y_max):\n",
    "                ax_x_min, ax_x_max=min(left, ax_x_min), max(right, ax_x_max)\n",
    "                ax_y_min, ax_y_max=min(bottom, ax_y_min), max(top, ax_y_max)\n",
    "                image=mpimg.imread(f'{dir}/{each_file}')\n",
    "                plt.imshow(image, extent=extent, cmap=cmap)\n",
    "\n",
    "    # set boundaries of the axis\n",
    "    ax.set_xlim([ax_x_min, ax_x_max])\n",
    "    ax.set_ylim([ax_y_min, ax_y_max])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5354b76-76b1-449a-99e3-924555cb5e93",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1-e2'></a>\n",
    "### Exercise #2 - Explore Tiles ###\n",
    "Let's explore the different tiles in our data set. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell to explore the data set. The regions where we have data for are `Bolivia`, `Paraguay`, `Somalia`, `Sri-Lanka`, `Ghana`, `India`, `USA`, `Spain`, `Pakistan`, `Mekong`, `Nigeria`. \n",
    "* Modify the `<FIXME>`s only and execute the cell below to explore the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf9f50-d774-4a36-9c8a-e09ab6aca9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_by_region('<<<<FIXME>>>>', '<<<<FIXME>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc37fe8-d4e7-47d2-a774-c23e4cd7dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_by_boundaries((<<<<FIXME>>>>, <<<<FIXME>>>>), (<<<<FIXME>>>>, <<<<FIXME>>>>), '<<<<FIXME>>>>')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ef0550a-8846-417e-869d-0ee8b7654474",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "tiles_by_region('Spain', 'images')\n",
    "tiles_by_region('Spain', 'masks')\n",
    "\n",
    "tiles_by_boundaries((-0.966, 38.4), (-0.597, 38.0), 'images')\n",
    "tiles_by_boundaries((-0.966, 38.4), (-0.597, 38.0), 'masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd3e38-a46e-4bba-94a2-a8735a3d93b5",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250abe6-9c9a-4aa3-b176-a4218656ffb1",
   "metadata": {},
   "source": [
    "<a name='s1-4'></a>\n",
    "## Data Pre-processing With DALI ##\n",
    "Deep learning models require vast amounts of data to produce accurate predictions, and this need becomes more significant as models grow in size and complexity. Regardless of the model, some degree of pre-processing is required for training and inference. In computer vision applications, the pre-processing usually includes decoding, resizing, and normalizing to a standardized format accepted by the neural network. Data preprocessing for deep learning workloads has garnered little attention until recently, eclipsed by the tremendous computational resources required for training complex models. These pre-processing routines, often referred to as pipelines, are currently executed on the CPU using libraries such as OpenCV, Pillow. Today’s DL applications include complex, multi-stage data processing pipelines consisting of many serial operations. Relying on the CPU to handle these pipelines have become a bottleneck that limits performance and scalability. \n",
    "<p><img src='images/dali.png' width=720></p>\n",
    "\n",
    "The **NVIDIA Data Loading Library** (DALI) is a library for data loading and pre-processing to accelerate deep learning applications. It provides a collection of highly optimized building blocks for loading and processing image, video, and audio data. DALI addresses the problem of the CPU bottleneck by offloading data preprocessing to the GPU. In addition, it offers some powerful features: \n",
    "* DALI offers data processing primitives for a variety of deep learning applications. The supported input formats include most used image file formats. \n",
    "* DALI relies on its own execution engine, built to maximize the throughput of the input pipeline. \n",
    "* It can be used as a portable drop-in replacement for built-in data loaders and data iterators in popular deep learning frameworks. \n",
    "* Features such as prefetching, parallel execution, and batch processing are handled transparently for the user. \n",
    "* Different deep learning frameworks have multiple data pre-processing implementations, resulting in challenges such as portability of training and inference workflows, and code maintainability. Data processing pipelines implemented using DALI are portable because they can easily be retargeted to TensorFlow, PyTorch, MXNet and PaddlePaddle.\n",
    "* Often the pre-processing routines that are used for inference are like the ones used for training, therefore implementing both using the same tools can save you some boilerplate and code repetition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8816a-f237-4963-bc59-887695846dbc",
   "metadata": {},
   "source": [
    "<a name='s1-4.1'></a>\n",
    "###  DALI Pipeline ###\n",
    "At the core of data processing with DALI lies the concept of a data processing `pipeline`. It is composed of multiple operations connected in a directed graph and contained in an object of class `nvidia.dali.Pipeline`. This class provides functions necessary for defining, building, and running data processing pipelines. Each operator in the pipeline typically gets one or more inputs, applies some kind of data processing, and produces one or more outputs. There are special kinds of operators that don’t take any inputs and produce outputs. Those special operators act like a data source – _readers_, _random number generators_ and _external source_ fall into this category. \n",
    "\n",
    "DALI offers CPU and GPU implementations for a wide range of processing operators. The availability of a CPU or GPU implementation depends on the nature of the operator. Make sure to check the documentation for an [up-to-date list of supported operations](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/#operations), as it is expanded with every release. The easiest way to define a DALI pipeline is using the `pipeline_def` Python [decorator](https://peps.python.org/pep-0318/). To create a pipeline, we define a function where we instantiate and connect the desired operators and return the relevant outputs. Then just decorate it with `pipeline_def`. Let's start with defining a very simple pipeline, which will have two operators. The first operator is a file reader that discovers and loads files contained in a directory. The reader outputs both the contents of the files (in this case, PNGs) and the labels, which are inferred from the directory structure. The second operator is an [image decoder](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/supported_ops.html#nvidia.dali.fn.decoders.image). Lastly, we return the image and label pairs. The easiest way to create a pipieline is by using the `pipeline_def` decorator. In the `simple_pipeline` function we define the operations to be performed and the flow of the computation between them. For more information about `pipeline_def` look to the [documentation](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html?#nvidia.dali.pipeline_def). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7efed4-c144-4b8b-9792-12e883859748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee8ff9-7c68-4da9-92e6-29ece14029d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "batch_size=4\n",
    "\n",
    "@pipeline_def\n",
    "def simple_pipeline():\n",
    "    # use fn.readers.file to read encoded images and labels from the hard drive\n",
    "    pngs, labels=fn.readers.file(file_root=image_dir)\n",
    "    # use the fn.decoders.image operation to decode images from png to RGB\n",
    "    images=fn.decoders.image(pngs, device='cpu')\n",
    "    # specify which of the intermediate variables should be returned as the outputs of the pipeline\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb5021-7748-413b-b8c4-df461323b578",
   "metadata": {},
   "source": [
    "In order to use the pipeline defined with `simple_pipeline`, we need to create and build it. This is achieved by calling `simple_pipeline()`, which creates an instance of the pipeline. Then we call `build()` on this newly created instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c40e00-bc94-4247-881b-d628926522f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create and build pipeline\n",
    "pipe=simple_pipeline(batch_size=batch_size, num_threads=4, device_id=0)\n",
    "pipe.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818566a0-384a-4e80-a94c-71962f7e0dea",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Notice that decorating a function with `pipeline_def` adds new named arguments to it. They can be used to control various aspects of the pipeline, such as batch size, number of threads used to perform computation on the CPU, and which GPU device to use (though pipeline created with simple_pipeline does not yet use GPU for compute). For more information about `Pipeline` arguments you can look to [Pipeline documentation](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/pipeline.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa08bd-1d97-417f-85af-9da07df19602",
   "metadata": {},
   "source": [
    "Once built, a pipeline instance runs in an [asynchronous](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)) fashion by calling the pipeline's `run()` method to get a batch of results. We unpack the results into `images` and `labels` as expected. Both of these elements contain a list of tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423c4f6-c163-4844-b52c-e9355cb6620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# run the pipeline\n",
    "simple_pipe_output=pipe.run()\n",
    "\n",
    "images, labels=simple_pipe_output\n",
    "print(\"Images is_dense_tensor: \" + str(images.is_dense_tensor()))\n",
    "print(\"Labels is_dense_tensor: \" + str(labels.is_dense_tensor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842e0f3-3b4a-40ce-a28d-5b30e3fa05b7",
   "metadata": {},
   "source": [
    "In order to see the images, we will need to loop over all tensors contained in `TensorList`, accessed with its `at` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d1498-3599-4153-bd8e-47765b4786b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define a function display images\n",
    "def show_images(image_batch):\n",
    "    columns=4\n",
    "    rows=1\n",
    "    # create plot\n",
    "    fig=plt.figure(figsize=(15, (15 // columns) * rows))\n",
    "    gs=gridspec.GridSpec(rows, columns)\n",
    "    for idx in range(rows*columns):\n",
    "        plt.subplot(gs[idx])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_batch.at(idx))\n",
    "    plt.tight_layout()\n",
    "\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9e180-cce2-4612-b0df-d34ca6ac4cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1-4.2'></a>\n",
    "### Data Augmentation ###\n",
    "Deep learning models require training with vast amounts of data to achieve accurate results. DALI can not only read images from disk and batch them into tensors, it can also perform various augmentations on those images to improve deep learning training results. [Data augmentation](https://en.wikipedia.org/wiki/Data_augmentation) artificially increases the size of a data set by introducing random disturbances to the data, such as _geometric deformations_, _color transforms_, _noise addition_, and so on. These disturbances help produce models that are more robust in their predictions, avoid overfitting, and deliver better accuracy. We will use DALI to demonstrate data augmentation that we will introduce for model training, such as _cropping_, _resizing_, and _flipping_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a836f27-298a-4c7c-8bc5-1a22788d331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import random\n",
    "\n",
    "@pipeline_def\n",
    "def augmentation_pipeline():\n",
    "    # use fn.readers.file to read encoded images and labels from the hard drive\n",
    "    image_pngs, _=fn.readers.file(file_root=image_dir)\n",
    "    # use the fn.decoders.image operation to decode images from png to RGB\n",
    "    images=fn.decoders.image(image_pngs, device='cpu')\n",
    "    \n",
    "    # the same augmentation needs to be performed on the associated masks\n",
    "    mask_pngs, _=fn.readers.file(file_root=mask_dir)\n",
    "    masks=fn.decoders.image(mask_pngs, device='cpu')\n",
    "    \n",
    "    image_size=512\n",
    "    roi_size=image_size*.5\n",
    "    roi_start_x=image_size*random.uniform(0, 0.5)\n",
    "    roi_start_y=image_size*random.uniform(0, 0.5)\n",
    "    \n",
    "    # use fn.resize to investigate an roi, region of interest\n",
    "    resized_images=fn.resize(images, size=[512, 512], roi_start=[roi_start_x, roi_start_y], roi_end=[roi_start_x+roi_size, roi_start_y+roi_size])\n",
    "    resized_masks=fn.resize(masks, size=[512, 512], roi_start=[roi_start_x, roi_start_y], roi_end=[roi_start_x+roi_size, roi_start_y+roi_size])\n",
    "    \n",
    "    # use fn.resize to flip the image\n",
    "    flipped_images=fn.resize(images, size=[-512, -512])\n",
    "    flipped_masks=fn.resize(masks, size=[-512, -512])\n",
    "    return images, resized_images, flipped_images, masks, resized_masks, flipped_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742719a-99e6-4cb9-a8c1-4bd10dc9147d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "pipe=augmentation_pipeline(batch_size=batch_size, num_threads=4, device_id=0)\n",
    "pipe.build()\n",
    "augmentation_pipe_output=pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d866b-fa78-41bd-817b-9a43e3a633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define a function display images\n",
    "def show_augmented_images(pipe_output):\n",
    "    image_batch, resized_image_batch, flipped_image_batch, mask_batch, resized_mask_batch, flipped_mask_batch=pipe_output\n",
    "    columns=6\n",
    "    rows=batch_size\n",
    "    # create plot\n",
    "    fig=plt.figure(figsize=(15, (15 // columns) * rows))\n",
    "    gs=gridspec.GridSpec(rows, columns)\n",
    "    grid_data=[image_batch, resized_image_batch, flipped_image_batch, mask_batch, resized_mask_batch, flipped_mask_batch]\n",
    "    grid=0\n",
    "    for row_idx in range(rows): \n",
    "        for col_idx in range(columns): \n",
    "            plt.subplot(gs[grid])\n",
    "            plt.axis('off')\n",
    "            plt.imshow(grid_data[col_idx].at(row_idx))\n",
    "            grid+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cf476-b68f-4094-87dd-4d6f534e0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "show_augmented_images(augmentation_pipe_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c3bfa-7ae3-4421-8d77-0eab593fe137",
   "metadata": {},
   "source": [
    "<a name='s1-e3'></a>\n",
    "### Exercise #3 - Data Augmentation on Batch ###\n",
    "Let's perform data augmentation on more batches of data. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Execute the cell to run the pipeline on the (next) batch of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf497a1-492f-4136-8220-02f2a99897fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "show_augmented_images(pipe.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692a278-06f3-4a23-84cd-dbf86b06186b",
   "metadata": {},
   "source": [
    "<a name='s1-4.3'></a>\n",
    "### Random Rotation ###\n",
    "Now let us perform additional data augmentation by as rotating each image (by a random angle). To generate a random angle, we can use `random.uniform`, and `rotate` for the rotation. We create another pipeline that uses the GPU to perform augmentations. DALI makes this transition very easy. The only thing that changes is the definition of the `rotate` operator. We only need to set the device argument to `gpu` and make sure that its input is transferred to the GPU by calling `.gpu()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae41dff-814b-4689-a0b8-7b231ef8c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "@pipeline_def\n",
    "def rotate_pipeline():\n",
    "    images, _=fn.readers.file(file_root=image_dir)\n",
    "    masks, _=fn.readers.file(file_root=mask_dir)\n",
    "    images=fn.decoders.image(images, device='cpu')\n",
    "    masks=fn.decoders.image(masks, device='cpu')\n",
    "    \n",
    "    angle=fn.random.uniform(range=(-30.0, 30.0))\n",
    "    rotated_images = fn.rotate(images.gpu(), angle=angle, fill_value=0, keep_size=True, device='gpu')\n",
    "    rotated_masks = fn.rotate(masks.gpu(), angle=angle, fill_value=0, keep_size=True, device='gpu')\n",
    "    \n",
    "    return rotated_images, rotated_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782be9c5-14ba-4c78-b1a2-72d913d604ad",
   "metadata": {},
   "source": [
    "The `rotate_pipeline` now performs the rotations on the GPU. Keep in mind that the resulting images are also allocated in the GPU memory, which is typically what we want, since the model requires the data in GPU memory. In any case, copying back the data to CPU memory after running the pipeline can be easily achieved by calling `as_cpu` on the objects returned by `Pipeline.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b345aa0-f7b7-4de9-a713-b51709bc4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "pipe=rotate_pipeline(batch_size=batch_size, num_threads=4, device_id=0)\n",
    "pipe.build()\n",
    "rotate_pipe_output= pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387ad9f-8e9f-4596-ab0d-4821162c23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# define a function display images\n",
    "def show_rotate_images(pipe_output):\n",
    "    image_batch, rotated_batch=pipe_output\n",
    "    columns=batch_size\n",
    "    rows=2\n",
    "    fig=plt.figure(figsize=(15, (15 // columns) * rows))\n",
    "    gs=gridspec.GridSpec(rows, columns)\n",
    "    grid_data=[image_batch.as_cpu(), rotated_batch.as_cpu()]\n",
    "    grid=0\n",
    "    for row_idx in range(rows): \n",
    "        for col_idx in range(columns): \n",
    "            plt.subplot(gs[grid])\n",
    "            plt.axis('off')\n",
    "            plt.imshow(grid_data[row_idx].at(col_idx))\n",
    "            grid+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cb9b7-5625-4f53-8509-ee0a18af3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "show_rotate_images(rotate_pipe_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fbd57-dcac-4844-ad7e-4e61a8957269",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](./02_efficient_model_training.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51627efc-8827-4d16-bc4d-c04e570f7062",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
