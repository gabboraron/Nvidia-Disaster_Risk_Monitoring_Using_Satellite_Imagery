{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c62cc95-69e4-4820-9a0c-e605ad87b25e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0863b8-d8e1-4ae8-88d8-154e69e14de1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Disaster Risk Monitoring Using Satellite Imagery #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e3b9e-1612-460a-a3b1-1560f0651441",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 - Deploying a Model for Inference ##\n",
    "In this notebook, we will take our previously segmentation model and deploy it on Triton Inference Server. TensorRT is a highly optimized package that takes trained models and optimizes them for inference. We'll see how to create model directory structures and configuration files within Triton Inference Server and how to send inference requests to the models deployed within Triton Inference Server.\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Introduction to Triton Inference Server](#s3-1)\n",
    "    * [Server](#s3-1.1)\n",
    "    * [Client](#s3-1.2)\n",
    "2. [Model Repository](#s3-2)\n",
    "    * [Exercise #1 - Model Configuration](#s3-e1)\n",
    "3. [Run Inference on Triton Inference Server](#s3-3)\n",
    "    * [Server Health Status](#s3-3.1)\n",
    "    * [Exercise #2 - Pre-process Inputs](#s3-e2)\n",
    "    * [Send Request to Server](#s3-3.2)\n",
    "    * [Visualize Results](#s3-3.3)\n",
    "4. [Conclusion](#s3-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1945095-75e2-4b72-b47c-d30ba6003d67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3-1'></a>\n",
    "## Introduction to Triton Inference Server ##\n",
    "NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) simplifies the deployment of AI models at scale in production. Triton is an open-source, inference-serving software that lets teams deploy trained AI models from any framework, from local storage, or from Google Cloud Platform or Azure on any GPU or CPU-based infrastructure, cloud, data center, or edge. The below figure shows the Triton Inference Server high-level architecture. The model repository is a _file-system based repository_ of the models that Triton will make available for inferencing. Inference requests arrive at the server via either [HTTP/REST](https://en.wikipedia.org/wiki/Representational_state_transfer), [gRPC](https://en.wikipedia.org/wiki/GRPC), or by the C API and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "<p><img src='images/triton_server_architecture.png' width='720'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5277-b33b-4a1e-96bb-1b5570b60647",
   "metadata": {},
   "source": [
    "<a name='s3-1.1'></a>\n",
    "### Server ###\n",
    "Setting up the Triton Inference Server requires software for the server and the client. One can get started with Triton Inference Server by pulling the [container](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver) from the NVIDIA NGC catalog. In this lab, we already have Triton Inference Server instance running. The code to run a Triton Server Instance is shown below. More details can be found in the QuickStart and build instructions:\n",
    "* [QuickStart Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/quickstart.md)\n",
    "* [Build Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/build.md)\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 \\\n",
    "  --ipc=host --rm \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /models:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:20.12-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b5615-d578-4fb8-8f61-d4eca4afa891",
   "metadata": {},
   "source": [
    "<a name='s3-1.2'></a>\n",
    "### Client ###\n",
    "We've also installed the Triton Inference Server Client libraries to provide APIs that make it easy to communicate with Triton from your C++ or Python application. Using these libraries, you can send either HTTP/REST or gRPC requests to Triton to access all its capabilities: inferencing, status and health, statistics and metrics, model repository management, etc. These libraries also support using system and CUDA shared memory for passing inputs to and receiving outputs from Triton. The easiest way to get the Python client library is to use `pip` to install the `tritonclient` module, as detailed below. For more details on how to download or build the Triton Inference Server Client libraries, you can find the documentation [here](https://github.com/triton-inference-server/server/blob/r20.12/docs/client_libraries.md), as well as examples that show the use of both the C++ and Python libraries.\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a84b9-2dd7-4ddb-8333-3add28b986ed",
   "metadata": {},
   "source": [
    "<a name='s3-2'></a>\n",
    "## Model Repository ##\n",
    "Triton Inference Server serves models within a model repository. When you first run Triton Inference Server, you'll specify the model repository where the models reside:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "Each model resides in its own model subdirectory within the model repository - i.e., each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying our `flood_segmentation_model`. All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g. _batch size_, _input shapes_, _deployment backend_ (PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. Additionally, we can create one or more versions of our model. Each version lives under a subdirectory name with the respective version number, starting with `1`. It is within this subdirectory where our model files reside. \n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── flood_segmentation_model\n",
    "│   ├── 1\n",
    "│   │   └── model.plan\n",
    "│   └── config.pbtxt\n",
    "│\n",
    "\n",
    "```\n",
    "\n",
    "We can also add a file representing the names of the outputs. We have omitted this step in this notebook for the sake of brevity. For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md). Below, we'll create the model directory structure for our flood detection segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625ce41-6668-4ddc-bee2-a57f71aa11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# set environment variables\n",
    "import os\n",
    "\n",
    "%set_env LOCAL_PROJECT_DIR=/dli/task/tao_project\n",
    "%set_env LOCAL_DATA_DIR=/dli/task/data\n",
    "\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"]=os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\"), \"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6839209-7b45-4972-8eb7-4f2adda8fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create directory for model\n",
    "!mkdir -p models/flood_segmentation_model/1\n",
    "\n",
    "# copy resnet18.engine from previous notebook to the model repository\n",
    "!cp $LOCAL_EXPERIMENT_DIR/export/resnet18.engine models/flood_segmentation_model/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f1991-163c-4e2b-b619-207d1d0dfbc4",
   "metadata": {},
   "source": [
    "<a name='s3-e1'></a>\n",
    "### Exercise #1 - Model Configuration ###\n",
    "With our model directory set up, we now turn our attention to creating the configuration file for our model. A minimal model configuration must specify the name of the model, the `platform` and/or backend properties, the `max_batch_size` property, and the `input` and `output` tensors of the model (name, data type, and shape). We can get the `output` tensor name from the `nvinfer_config.txt` [file](tao_project/unet/resnet18/weights/nvinfer_config.txt) we generated before under `output-blob-names`. For more details on how to create model configuration files within Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md). \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell to create the `config.pbtxt` file for the segmentation_model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d77ca4-6d6a-4a22-b9f8-2b7689232a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "configuration = \"\"\"\n",
    "name: \"flood_segmentation_model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input: [\n",
    " {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, <<<<FIXME>>>>, <<<<FIXME>>>> ]\n",
    "  }\n",
    "]\n",
    "output: {\n",
    "    name: \"<<<<FIXME>>>>\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ <<<<FIXME>>>>, <<<<FIXME>>>>, 2 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/flood_segmentation_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16ec86ef-8093-4eb1-b84c-f7342fd37418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "configuration = \"\"\"\n",
    "name: \"flood_segmentation_model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input: [\n",
    " {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "output: {\n",
    "    name: \"softmax_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 512, 512, 2 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/flood_segmentation_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30792e9f-97da-4c79-8bed-ef33b87a4215",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ba239-eb7d-4c72-b8c7-9b70074ba545",
   "metadata": {},
   "source": [
    "<a name='s3-3'></a>\n",
    "## Run Inference on Triton Inference Server ##\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding. Due to the asynchronous nature of this step, we have added 15 seconds to be safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3800be-ce51-4bcc-a353-fdcf03806f21",
   "metadata": {},
   "source": [
    "<a name='s3-3.1'></a>\n",
    "### Server Health Status ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4645417-1f93-4456-b6e3-0c7c2e7b3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49aeb80-4b61-43c9-a194-f4dc0ad00e02",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can send a `curl` request to the below URL. The HTTP request returns status _200_ if Triton is ready and _non-200_ if it is not ready. We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. Additionally, we will also see information about our models such:\n",
    "* The name of our model,\n",
    "* The versions available for our model,\n",
    "* The backend platform (e.g., tensort_rt, pytorch_libtorch, onnxruntime_onnx), \n",
    "* The inputs and outputs, with their respective names, data types, and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eb618-7d6b-48d2-8937-2a8b37ab072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626d1a8-ba1c-446c-a53d-891ed78d1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/models/flood_segmentation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62621e9b-92b0-432b-8ef2-eef7dd299432",
   "metadata": {},
   "source": [
    "<a name='s3-e2'></a>\n",
    "### Exercise #2 - Pre-process Inputs ###\n",
    "Triton itself does not do anything with your input tensors, it simply feeds them to the model. Same for outputs. Ensuring that the preprocessing operations used for inference are defined identically as they were when the model was trained is key to achieving high accuracy. In our case, we need to perform normalization and mean subtraction to produce the final float planar data to the TensorRT engine for inferencing. We can get the `offsets` and `net-scale-factor` from the `nvinfer_config.txt` [file](tao_project/unet/resnet18/weights/nvinfer_config.txt). The pre-processing function is:\n",
    "\n",
    "<b>y = net scale factor * (x-mean)</b>\n",
    "\n",
    "where: \n",
    "* x is the input pixel value. It is an int8 with range [0,255].\n",
    "* mean is the corresponding mean value, read either from the mean file or as offsets[c], where c is the channel to which the input pixel belongs, and offsets is the array specified in the configuration file. It is a float.\n",
    "* net-scale-factor is the pixel scaling factor specified in the configuration file. It is a float.\n",
    "* y is the corresponding output pixel value. It is a float.\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Execute the cell to import dependencies. \n",
    "* Modify the `<FIXME>`s only and execute the cell below to pre-process the input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8df2234-c035-4200-9a0a-5e0e7772de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317051a-0743-433c-a9a1-259874b5b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose random image\n",
    "random_image_file=random.sample(os.listdir(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images')), 1)\n",
    "\n",
    "# pre-process image\n",
    "image=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images', random_image_file[0]))\n",
    "image_ary=np.asarray(image)\n",
    "image_ary=image_ary.astype(np.float32)\n",
    "\n",
    "image_ary=(image_ary-<<<<FIXME>>>>)*<<<<FIXME>>>>\n",
    "image_ary=np.transpose(image_ary, [2, 0, 1])\n",
    "display(image_ary.shape)\n",
    "\n",
    "image_ary=np.expand_dims(image_ary, axis=0)\n",
    "display(image_ary.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e45856dc-48b0-4006-b616-6b741bea02c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# choose random image\n",
    "random_image_file=random.sample(os.listdir(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images')), 1)\n",
    "\n",
    "# pre-process image\n",
    "image=Image.open(os.path.join(os.getenv('LOCAL_DATA_DIR'), 'images', 'all_images', random_image_file[0]))\n",
    "image_ary=np.asarray(image)\n",
    "image_ary=image_ary.astype(np.float32)\n",
    "\n",
    "image_ary=(image_ary-127.5)*0.00784313725490196\n",
    "image_ary=np.transpose(image_ary, [2, 0, 1])\n",
    "display(image_ary.shape)\n",
    "\n",
    "image_ary=np.expand_dims(image_ary, axis=0)\n",
    "display(image_ary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10ef11-8edd-48a0-9da1-af960b5070a4",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558b9e-43d7-4d2a-8f9f-3a87129f7ca5",
   "metadata": {},
   "source": [
    "<a name='s3-3.2'></a>\n",
    "### Send Inference Request to Server ###\n",
    "With our models deployed, it is now time to send inference requests to our models. First, we'll load the `tritonclient.http` module. We will also define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case the host `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a18b3-ecdc-45c9-95b2-a5d8d3681646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import tritonclient.http as tritonhttpclient\n",
    "\n",
    "# set parameters\n",
    "VERBOSE=False\n",
    "input_name='input_1'\n",
    "input_shape=(1, 3, 512, 512)\n",
    "input_dtype='FP32'\n",
    "output_name='softmax_1'\n",
    "model_name='flood_segmentation_model'\n",
    "url='triton:8000'\n",
    "model_version='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d03260-a4e7-4668-8cd8-0d88a371562b",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f84f5-0c7a-4169-aa78-81283c2bf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "triton_client=tritonhttpclient.InferenceServerClient(url=url, verbose=VERBOSE)\n",
    "model_metadata=triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "model_config=triton_client.get_model_config(model_name=model_name, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ca85d-f46f-46bc-b0b0-27a965ebb4c8",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our image. We'll also instantiate a placeholder for our output data using just the output name. Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4884f-3187-4725-972b-f26aa5e80758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "inference_input=tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "inference_input.set_data_from_numpy(image_ary)\n",
    "\n",
    "output=tritonhttpclient.InferRequestedOutput(output_name)\n",
    "response=triton_client.infer(model_name, \n",
    "                             model_version=model_version, \n",
    "                             inputs=[inference_input], \n",
    "                             outputs=[output])\n",
    "logits=response.as_numpy(output_name)\n",
    "logits=np.asarray(logits, dtype=np.float32)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e07d2-b8de-4229-b439-d05b2a19a592",
   "metadata": {},
   "source": [
    "And that's all there is to it! We can identify the largest logit value per pixel and confirm that our model correctly inferred the presence of flood. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425b3d9-eefe-4350-8fb5-e2471bbc7d25",
   "metadata": {},
   "source": [
    "<a name='s3-3.3'></a>\n",
    "### Visualize Results ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db686646-4d0f-49ee-88bf-29f39588c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# visualize results\n",
    "fig, ax_arr=plt.subplots(1, 2, figsize=[15, 5], sharex=True, sharey=True)\n",
    "ax_arr[0].set_title('Input Data')\n",
    "ax_arr[1].set_title('Inference')\n",
    "ax_arr[0].set_xticks([])\n",
    "ax_arr[0].set_yticks([])\n",
    "\n",
    "ax_arr[0].imshow(image)\n",
    "ax_arr[1].imshow(np.argmax(logits[0], axis=2)*255, cmap='gray')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc05b4c-c28c-4438-8522-871fd3b71aca",
   "metadata": {},
   "source": [
    "Recall that we used a random input image for demonstration. In case you don't get an image that shows presence of flood, please feel free to re-run the inference starting from [Exercise #2](#s3-e2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86195126-f9a5-4a49-afc0-84f4101c66d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s3-4'></a>\n",
    "## Conclusion ##\n",
    "Once deployed, the Triton Inference Server can be connected to front-end applications such as those that power https://www.balcony.io/, which provides an emergency management platform that has the ability to send messages to personal devices. In terms of making the model better, improving on metrics like Intersect-Over-Union (IoU) translates to accurate flood modeling, and coupled with a time-optimized solution aids in real-time disaster response and eventual climate action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dc84b-ccd8-4295-87cf-897090d19c42",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
