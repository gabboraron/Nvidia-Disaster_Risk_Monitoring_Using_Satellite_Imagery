{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f36efa-b9f8-4935-a403-a4010cdbad8e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ee592-705e-49c5-a26c-dd7cacfbb545",
   "metadata": {},
   "source": [
    "# Disaster Risk Monitoring Using Satellite Imagery #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a48909-8799-4b25-bd11-91cc92fb46a1",
   "metadata": {},
   "source": [
    "## 02 - Efficient Model Training ##\n",
    "In this notebook, you will learn how to train a segmentation model with the TAO Toolkit using pre-trained Resnet-18 weights. In addition, you will learn how to export the model for deployment. \n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Introduction to TAO Toolkit](#s2-1)\n",
    "    * [Transfer Learning](#s2-1.1)\n",
    "    * [Vision AI Pre-trained Models Supported](#s2-1.2)\n",
    "    * [TAO Toolkit Workflow](#s2-1.3)\n",
    "    * [TAO Launcher, CLI (Command Line Interface), and Spec Files](#s2-1.4)\n",
    "    * [Exercise #1 - Explore TAO Toolkit CLI](#s2-e1)\n",
    "2. [U-Net Semantic Segmentation Model](#s2-2)\n",
    "    * [Preparation for Model Training](#s2-2.1)\n",
    "    * [Set Up Environment Variables](#s2-2.2)\n",
    "    * [Download Pre-trained Model](#s2-2.3)\n",
    "    * [Prepare Data Set](#s2-2.4)\n",
    "    * [Model Training](#s2-2.5)\n",
    "    * [Exercise #2 - Modify Data Set Config](#s2-e2)\n",
    "    * [Exercise #3 - Modify Model Config](#s2-e3)\n",
    "    * [Exercise #4 - Modify Training Config](#s2-e4)\n",
    "    * [Combine Configuration Files](#s2-2.6)\n",
    "    * [Initiate Model Training](#s2-2.7)\n",
    "    * [Evaluating the Model](#s2-2.8)\n",
    "    * [Visualizing Model Inference](#s2-2.9)\n",
    "3. [Model Export](#s2-3)\n",
    "    * [TensorRT - Programmable Inference Accelerator](#s2-3.1)\n",
    "    * [Export the Trained Model](#s2-3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae3a29-df70-4396-a6f4-d1d8d164d5ff",
   "metadata": {},
   "source": [
    "<a name='s2-1'></a>\n",
    "## Introduction to the TAO Toolkit ##\n",
    "The TAO Toolkit, Train Adapt Optimize, is a framework that simplifies the AI/ML model development workflow. It lets developers fine-tune pretrained models with custom data to produce highly accurate computer vision models efficiently, eliminating the need for large training runs and deep AI expertise. In addition, it also enables model optimization for inference performance. \n",
    "<p><img src=\"images/tao_toolkit.png\" width=720></p>\n",
    "\n",
    "The TAO Toolkit uses pre-trained models to accelerate the AI development process and reduce costs associated with large scale data collection, labeling, and training models from scratch. Transfer learning with pre-trained models can be used for classification, object detection, and image segmentation tasks. The TAO Toolkit offers useful features such as: \n",
    "* Zero-coding approach that requires no AI framework expertise, reducing the barrier of entry for anyone who wants to get started building AI-based applications\n",
    "* Flexible configurations that allow customization to help advance users prototype faster\n",
    "* Large catalogue of production-ready pre-trained models for common CV tasks that can also be customized with users' own data\n",
    "* Easy to use interface for model optimization such as pruning and quantization-aware training\n",
    "* Integration with the Triton Inference Server\n",
    "\n",
    "_Note: The TAO Toolkit comes with a set of reference scripts and configuration specifications with default parameter values that enable developers to kick-start training and fine-tuning. This lowers the bar and enables users without a deep understanding of models, expertise in deep learning, or beginning coding skills to be able to train new models and fine-tune the pretrained ones._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8aca25-bc7a-4dc7-a4d3-b903cbdf13f4",
   "metadata": {},
   "source": [
    "<a name='s2-1.1'></a>\n",
    "### Transfer Learning ###\n",
    "In practice, it is rare and inefficient to initiate the learning task on a network with randomly initialized weights due to factors like data scarcity (inadequate number of training samples) or prolonged training times. One of the most common techniques to overcome this is to use transfer learning. Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where developers use a model trained on one task and re-train to use it on a different task. This works surprisingly well as many of the early layers in a neural network are the same for similar tasks. For example, many of the early layers in a convolutional neural network used for a Computer Vision (CV) model are primarily used to identify outlines, curves, and other features in an image. The network formed by these layers are referred to as the **backbone** of a more complex model. Also known as feature extractors, they take as input the image and extracts the feature map upon which the rest of the network is based. The learned features from these layers can be applied to similar tasks carrying out the same identification in other domains. Transfer learning enables adaptation (fine-tuning) of an existing neural network to a new one, which requires significantly less domain-specific data. In most cases, fine-tuning takes significantly less time (a reduction by x10 factor is common), saving time and resources. As it relates to vision AI, transfer learning is used for scene adaptation by transferring weights from one application to another, adapting to a new point of view or a camera angle. Transfer learning is also used for adding new classifications. \n",
    "\n",
    "<p><img src='images/transfer_learning.png' width=720></p>\n",
    "\n",
    "More information about transfer learning can be found in this [blog](https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb01061-8a05-4c4f-96aa-28926278b503",
   "metadata": {},
   "source": [
    "<a name='s2-1.2'></a>\n",
    "### Vision AI Pre-trained Models Supported ###\n",
    "Developers, system builders, and software partners building disaster risk monitoring systems can bring their own custom data to train with and fine-tune pre-trained models quickly instead of going through significant effort in large data collection and training from scratch. **General purpose vision models** provide pre-trained weights for popular network architectures to train an image classification model, an object detection model, or a segmentation model. This gives users the flexibility and control to build AI models for any number of applications, from smaller lightweight models for edge deployment to larger models for more complex tasks. They are trained on [Open Images](https://opensource.google/projects/open-images-dataset) data set and provide a much better starting point for training versus training from scratch or starting from random weights. \n",
    "\n",
    "The TAO Toolkit adapts popular network architectures and backbones to custom data, allowing developers to train, fine tune, prune, and export highly optimized and accurate AI models. When working with TAO, first choose the model architecture to be built, then choose one of the supported backbones. \n",
    "<p><img src='images/tao_matrix.png' width=720></p>\n",
    "\n",
    "_Note: The pre-trained weights from each feature extraction network merely act as a starting point and may not be used without re-training. In addition, the pre-trained weights are network specific and shouldn't be shared across models that use different architectures._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319ca49-760b-4aa1-b2f5-8c31657c125c",
   "metadata": {},
   "source": [
    "<a name='s2-1.3'></a>\n",
    "### TAO Toolkit Workflow ###\n",
    "Building disaster risk monitoring systems is hard. And tailoring even a single component to the needs of the enterprise for deployment is even harder. Deployment for a domain-specific application typically requires several cycles of re-training, fine-tuning, and deploying the model until it satisfies the requirements. As a starting point, training typically follows the below steps: \n",
    "\n",
    "0. Configuration\n",
    "1. Download a pre-trained model from NGC\n",
    "2. Prepare the data for training\n",
    "3. Train the model using transfer learning\n",
    "4. Evaluate the model for target predictions\n",
    "5. Export the model\n",
    "* Steps to optimize the model for improved inference performance\n",
    "\n",
    "<p><img src='images/tao_toolkit_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb69288-90df-455f-b53a-0a32adf74dbc",
   "metadata": {},
   "source": [
    "<a name='s2-1.4'></a>\n",
    "### TAO Launcher, CLI (Command Line Interface), and Spec Files ###\n",
    "The TAO Toolkit is a zero-coding framework that makes it easy to get started. It uses a **launcher** to pull from NGC registry and instantiate the appropriate TAO container that performs the desired subtasks such as convert data, train, evaluate, or export. The TAO launcher is a python package distributed as a python wheel listed in the `nvidia-pyindex` python index, which has been prepared for you already. \n",
    "\n",
    "Users interact with the launcher with its **Command Line Interface** that is configured using simple [**Protocol Buffer**](https://developers.google.com/protocol-buffers) **specification files** to include parameters such as the data set parameters, model parameters, and optimizer and training hyperparameters. More information about the TAO Toolkit Launcher can be found in the [TAO Docs](https://docs.nvidia.com/tao/tao-toolkit/text/tao_launcher.html#tao-launcher). \n",
    "\n",
    "The tasks can be invoked from the TAO Toolkit Launcher using the convention `tao <task> <subtask> <args_per_subtask>`, where `<args_per_subtask>` are the arguments required for a given subtask. Once the container is launched, the subtasks are run by the TAO Toolkit containers using the appropriate hardware resources. \n",
    "<p><img src='images/tao_launcher.gif' width=720></p>\n",
    "\n",
    "To see the usage of different functionality that are supported, use the `-h` or `--help` option. For more information, see the [TAO Toolkit Quick Start Guide](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html). \n",
    "Here is the **sample output**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c2ed9-6c06-4e74-aa9e-1dc3034228c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24d50a-f760-483c-90e0-680a45869b26",
   "metadata": {},
   "source": [
    "With the TAO Toolkit, users can train models for object detection, classification, segmentation, optical character recognition, facial landmark estimation, gaze estimation, and more. In TAO's terminology, these would be the **tasks**, which support **subtasks** such as `train`, `prune`, `evaluate`, `export`, etc. Each task/subtask requires different combinations of configuration files to accommodate for different parameters, such as the dataset parameters, model parameters, and optimizer and training hyperparameters. Part of what makes TAO Toolkit so easy to use is that most of those parameters are hidden away in the form of experiment specification files (spec files). They are detailed in the [Getting Started Guide](https://docs.nvidia.com/tao/archive/tlt-10/pdf/Transfer-Learning-Toolkit-Getting-Started-Guide-IVA.pdf) for reference. It's very helpful to have these resources handy when working with the TAO Toolkit. In addition, there are several specific tasks that help with handling the launched commands. Below is a list of available options for task. We grayed out the tasks for Conversational AI as they are out of scope for this course. \n",
    "\n",
    "<img src='images/tao_tasks.png' width=740>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81e20c-14fa-4b8f-a630-ed8d9a509b59",
   "metadata": {},
   "source": [
    "<a name='s2-e1'></a>\n",
    "### Exercise #1 - Explore TAO Toolkit CLI ###\n",
    "Let's explore some TAO Toolkit tasks. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell, choosing a task from options such as: `[classification, detectnet_v2, mask_rcnn, emotionnet, etc.]`, follow by a subtask from options such as: `[calibration_tensorfile, dataset_convert, evaluate, export, inference, prune, train]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6698ea-f544-417e-9f19-9f205b473e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: !tao unet train --help\n",
    "!tao <<<<FIXME>>>> <<<<FIXME>>>> --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfad17-e606-4565-b3e3-329538b2bc83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao unet train --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf953065-06a7-491f-9fef-f5c2c24f7f8b",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f19dad-30cb-4d12-a15d-7268743ab2b3",
   "metadata": {},
   "source": [
    "<p><img src='images/check.png' width=720></p>\n",
    "Did you get the below error message? This is likely due to a bad NGC configuration. Please check the NGC CLI and Docker Registry section of the introduction notebook."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff968ed5-77ee-4237-8709-f55e96cc70b0",
   "metadata": {},
   "source": [
    "AssertionError: Config path must be a valid unix path. No file found at: /root/.docker/config.json. Did you run docker login?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f932728-0ae0-40bf-82b7-32f7b935cf8d",
   "metadata": {},
   "source": [
    "<a name='s2-2'></a>\n",
    "## U-Net Semantic Segmentation Model ##\n",
    "[U-Net](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pretrained_semantic_segmentation) is a network for image segmentation. This is the type of task we want to perform for our disaster risk monitoring system in order to label each pixel as either `flood` or `notflood`. With the TAO Toolkit, we can choose the desired ResNet-18 backbone as a feature extractor. As such, we will use the `unet` task, which supports the following subtasks: \n",
    "* `train`\n",
    "* `evaluate`\n",
    "* `inference`\n",
    "* `prune`\n",
    "* `export`\n",
    "\n",
    "<p><img src='images/rewind.png' width=720><p>\n",
    "    \n",
    "These subtasks can be invoked using the convention `tao unet <subtask> <args_per_subtask>` on the command-line, where `args_per_subtask` are the arguments required for a given subtask. Additionally, we can always find more information about these subtasks with `tao unet <subtask> -h` or `tao unet <subtask> --help`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281849b7-b93b-4d41-9437-8e34a2953087",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-2.1'></a>\n",
    "### Preparation for Model Training ###\n",
    "For the remaining of the lab, we will use the TAO Toolkit to train a semantic segmentation model. Below is what the model development workflow looks like. We start by preparing a pre-trained model and the data. Next, we prepare the configuration file(s) and begin to train the model with new data and evaluate its performance. We will export the model once its satisfactory. Note that this notebook does not include inference optimization steps, which is important for disaster risk monitoring systems that are deployed on edge devices. \n",
    "<p><img src='images/simple_workflow.png' width=720></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7f4ec-1826-499f-b001-28d1eb0d71a1",
   "metadata": {},
   "source": [
    "<a name='s2-2.2'></a>\n",
    "### Set Up Environment Variables ###\n",
    "We set up a couple of environment variables to help us mount the local directories to the tao container. Specifically, we want to set paths for the `$LOCAL_TRAINING_DATA`, `$LOCAL_SPEC_DIR`, and `$LOCAL_PROJECT_DIR` for the output of the TAO experiment with their respective paths in the TAO container. In doing so, we can make sure that the TAO experiment generated collaterals such as checkpoints, model files (e.g. `.tlt` or `.etlt`), and logs are output to `$LOCAL_PROJECT_DIR/unet`. \n",
    "\n",
    "_Note that users will be able to define their own export encryption key when training from a general-purpose model. This is to protect proprietary IP and used to decrypt the `.etlt` model during deployment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446943db-e633-4b5b-9997-c767a849085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# set environment variables\n",
    "import os\n",
    "\n",
    "%set_env KEY=my_model_key\n",
    "\n",
    "%set_env LOCAL_PROJECT_DIR=/dli/task/tao_project\n",
    "%set_env LOCAL_DATA_DIR=/dli/task/data\n",
    "%set_env LOCAL_SPECS_DIR=/dli/task/tao_project/spec_files\n",
    "os.environ[\"LOCAL_EXPERIMENT_DIR\"]=os.path.join(os.getenv(\"LOCAL_PROJECT_DIR\"), \"unet\")\n",
    "\n",
    "%set_env TAO_PROJECT_DIR=/workspace/tao-experiments\n",
    "%set_env TAO_DATA_DIR=/workspace/tao-experiments/data\n",
    "%set_env TAO_SPECS_DIR=/workspace/tao-experiments/spec_files\n",
    "%set_env TAO_EXPERIMENT_DIR=/workspace/tao-experiments/unet\n",
    "\n",
    "!mkdir $LOCAL_EXPERIMENT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe38326-8dc7-438f-b392-913166d3a185",
   "metadata": {},
   "source": [
    "The cell below maps the project directory on your local host to a workspace directory in the TAO docker instance, so that the data and the results are mapped from in and out of the docker. This is done by creating a `.tao_mounts.json` file. For more information, please refer to the [launcher instance](https://docs.nvidia.com/tao/tao-toolkit/tao_launcher.html) in the user guide. Setting the `DockerOptions` ensures that you don't have permission issues when writing data into folders created by the TAO docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a30e1d-a66d-4752-85d7-6676f31c3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# mapping up the local directories to the TAO docker\n",
    "import json\n",
    "mounts_file = os.path.expanduser(\"~/.tao_mounts.json\")\n",
    "\n",
    "drive_map = {\n",
    "    \"Mounts\": [\n",
    "            # Mapping the data directory\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_PROJECT_DIR\"],\n",
    "                \"destination\": \"/workspace/tao-experiments\"\n",
    "            },\n",
    "            # Mapping the specs directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_SPECS_DIR\"],\n",
    "                \"destination\": os.environ[\"TAO_SPECS_DIR\"]\n",
    "            },\n",
    "            # Mapping the data directory.\n",
    "            {\n",
    "                \"source\": os.environ[\"LOCAL_DATA_DIR\"],\n",
    "                \"destination\": os.environ[\"TAO_DATA_DIR\"]\n",
    "            },\n",
    "        ],\n",
    "    \"DockerOptions\": {\n",
    "        \"user\": \"{}:{}\".format(os.getuid(), os.getgid())\n",
    "    }\n",
    "}\n",
    "\n",
    "# writing the mounts file\n",
    "with open(mounts_file, \"w\") as mfile:\n",
    "    json.dump(drive_map, mfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bde40-85ba-472c-9984-6222fbf4d3d9",
   "metadata": {},
   "source": [
    "<a name='s2-2.3'></a>\n",
    "### Download Pre-trained Model ###\n",
    "Developers typically begin by choosing and downloading a pre-trained model from [NGC](https://ngc.nvidia.com/) which contains pre-trained weights of the architecture of their choice. It's difficult to immediately identify which model/architecture will work best for a specific use case as there is often a tradeoff between time to train, accuracy, and inference performance. It is common to compare across multiple models before picking the best candidate.\n",
    "\n",
    "Here are some pointers that will help choose an appropriate model: \n",
    "* Look at the model inputs/outputs to decide if it will fit your use case. \n",
    "* Input format is also an important consideration. For example, some models expect the input to be 0-1 normalized with input channels in RGB order. Some models that use a different input order may require input preprocessing/mean subtraction that might result in suboptimal performance. \n",
    "\n",
    "We can use the `ngc registry model list <model_glob_string>` command to get a list of models that are hosted on the NGC model registry. For example, we can use `ngc registry model list nvidia/tao/*` to list all available models. The `--column` option identifies the columns of interest. More information about the NGC Registry CLI can be found in the [User Guide](https://docs.nvidia.com/dgx/pdf/ngc-registry-cli-user-guide.pdf). The `ngc registry model download-version <org>/[<team>/]<model-name:version>` command will download the model from the registry. It has a `--dest` option to specify the path to download directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a84c4-b633-41d1-a34d-7cd70282f348",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# list all available models\n",
    "!ngc registry model list nvidia/tao/pretrained_semantic_segmentation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334806b-5dd7-4c36-b1a4-b2bfe2620ffe",
   "metadata": {},
   "source": [
    "<p><img src='images/check.png' width=720></p>\n",
    "Did you get the below error message? This is likely due to a bad NGC CLI configuration. Please check the NGC CLI and Docker Registry section of the introduction notebook."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d0d9d37-d76a-420e-bfea-f1a05e902488",
   "metadata": {},
   "source": [
    "{\n",
    "    \"error\": \"Error: Invalid apikey\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd94e07-ae7f-47a1-9955-7b4415fa0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# create directory to store the pre-trained model\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/pretrained_resnet18/\n",
    "\n",
    "# download the pre-trained segmentation model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_semantic_segmentation:resnet18 \\\n",
    "    --dest $LOCAL_EXPERIMENT_DIR/pretrained_resnet18 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e1652-fe1f-4a17-9307-0d2353ab1945",
   "metadata": {},
   "source": [
    "<a name='s2-2.4'></a>\n",
    "### Prepare Data Set ###\n",
    "The TAO Toolkit expects the training data for the `unet` task to be in the format described in the [documentation](https://docs.nvidia.com/tao/tao-toolkit/text/data_annotation_format.html#id8). `unet` expects the images and corresponding masks encoded as images. Each mask image is a single-channel image, where every pixel is assigned an integer value that represents the segmentation class. Additionally, each image and label have the same file ID before the extension. The image-to-label correspondence is maintained using this filename. The data folder structure for images and masks must be in the following format. \n",
    "<p><img src='images/semantic_segmentation_input.PNG' width=720></p>\n",
    "\n",
    "_The `test` folder in the above directory structure is optional; any folder can be used for inference._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ebbf0-1e1b-470e-9c3b-7fd9fcc62906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove existing splits\n",
    "!rm -rf $LOCAL_DATA_DIR/images/train\n",
    "!mkdir -p $LOCAL_DATA_DIR/images/train\n",
    "!rm -rf $LOCAL_DATA_DIR/images/val\n",
    "!mkdir -p $LOCAL_DATA_DIR/images/val\n",
    "\n",
    "!rm -rf $LOCAL_DATA_DIR/masks/train\n",
    "!mkdir -p $LOCAL_DATA_DIR/masks/train\n",
    "!rm -rf $LOCAL_DATA_DIR/masks/val\n",
    "!mkdir -p $LOCAL_DATA_DIR/masks/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3a078-58db-42a6-b873-56d87f024ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "from random import sample\n",
    "import shutil\n",
    "# define split ratio\n",
    "split=0.75\n",
    "\n",
    "# get all images\n",
    "file_list=os.listdir(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images\")\n",
    "image_count=len(file_list)\n",
    "train_image_list=sample(file_list, int(image_count*split))\n",
    "val_image_list=[file for file in file_list if file not in train_image_list]\n",
    "\n",
    "# move all training images to train directory\n",
    "for each_file in train_image_list: \n",
    "    if each_file.split('.')[-1]=='png': \n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/images/train/{each_file}\")\n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/masks/all_masks/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/masks/train/{each_file}\")\n",
    "\n",
    "# move all validation images to val directory\n",
    "for each_file in val_image_list: \n",
    "    if each_file.split('.')[-1]=='png': \n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/images/all_images/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/images/val/{each_file}\")\n",
    "        shutil.copyfile(f\"{os.environ['LOCAL_DATA_DIR']}/masks/all_masks/{each_file}\", f\"{os.environ['LOCAL_DATA_DIR']}/masks/val/{each_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b710aa-c92c-4d09-934f-5096b2711965",
   "metadata": {},
   "source": [
    "<a name='s2-2.5'></a>\n",
    "### Model Training ###\n",
    "Training configuration is done through a training spec file, which includes options such as which data set to use for training, which data set to use for validation, which pre-trained model architecture to use, which hyperparameters to tune, and other training options. The `train`, `evaluate`, `prune`, and `inference` subtasks for an U-Net experiment share the same configuration file. Configuration files can be created from scratch or modified using the templates provided in TAO Toolkit's [sample applications](https://docs.nvidia.com/tao/tao-toolkit/#cv-applications). \n",
    "\n",
    "The training configuration file has eight sections: \n",
    "* `dataset_config`\n",
    "* `model_config`\n",
    "* `training_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb34518-d2fd-449c-bcbf-fa5b2fcd086f",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "We will create the configuration files using templates. Specifically, we have broken the configuration files into separate parts for ease of discussion, which we will combine at the end for the TAO Toolkit to consume. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c4241d-14b2-44fb-8820-83c3d61f93cd",
   "metadata": {},
   "source": [
    "Execute the below cells to preview the combined training/evaluation configuration file that will be used. It is currently not usable as we have made some intentional modifications that will require correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3201c-2443-4f94-852f-f15cc51b25e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# combining configuration components in separate files and writing into one\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/model_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/training_config.txt \\\n",
    "     > $LOCAL_SPECS_DIR/resnet18/combined_config.txt\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/combined_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ded461-e7f3-4d20-b406-e1a0b309eb29",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Note that we must leave an empty new line at the end of each text file to ensure the `combined_config.txt` is created properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b587b-d099-48ab-be87-3e3c8b3f41af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-e2'></a>\n",
    "### Exercise #2 - Modify Data Set Config ###\n",
    "The dataloader defines the path to the data to be trained on and the class mapping for the classes in the data set. We have previously generated images and masks for the training data sets. To use the newly generated training data, update the `dataset_config` parameter in the spec file to reference the correct directory. \n",
    "* `dataset (str)`: The input type dataset used. The currently supported dataset is `custom` to the user. Open-source datasets will be added in the future. \n",
    "* `augment (bool)`: If the input should augmented online while training. When using one’s own data set to train and fine-tune a model, the data set can be augmented while training to introduce variations in the data set. This is known as **online augmentation**. This is very useful in training as data variation improves the overall quality of the model and prevents [overfitting](https://en.wikipedia.org/wiki/Overfitting). Training a deep neural network requires large amounts of annotated data, which can be a manual and expensive process. Furthermore, it can be difficult to estimate all the corner cases that the network may go through. The TAO Toolkit provides _spatial augmentation_ (resize and flip) and _color space augmentation_ (brightness) to create synthetic data variations. \n",
    "* `augmentation_config (dict)`: \n",
    "    * `spatial_augmentation (dict)`: Supports spatial augmentation such as flip, zoom, and translate. \n",
    "        * `hflip_probability (float)`: Probability to flip an input image horizontally. \n",
    "        * `vflip_probability (float)`: Probability to flip an input image vertically. \n",
    "        * `crop_and_resize_prob (float)`\n",
    "    * `brightness_augmentation (dict)`: Configures the color space transformation. \n",
    "        * `delta (float)`: Adjust brightness using delta value. \n",
    "* `input_image_type (str)`: The input image type to indicate if input image is `grayscale` or `color` (RGB). \n",
    "* `train_images_path (str)`, `train_masks_path (str)`, `val_images_path (str)`, `val_masks_path (str)`, `test_images_path (str)`: The path string for train images, train masks, validation images, validation masks, and test images (optional). \n",
    "* `data_class_config (dict)`: Proto dictionary that contains information of training classes as part of target_classes proto which is described below.\n",
    "    * `target_classes (dict)`: The repeated field for every training class. The following are required parameters for the target_classes config:\n",
    "        * `name (str)`: The name of the target class. \n",
    "        * `mapping_class (str)`: The name of the mapping class for the target class. If the class needs to be trained as is, then name and mapping_class should be the same.\n",
    "        * `label_id (int)`: The pixel that belongs to this target class is assigned this label_id value in the mask image.\n",
    "\n",
    "_Note the supported image extension formats for training images are “.png”, “.jpg”, “.jpeg”, “.PNG”, “.JPG”, and “.JPEG”._\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `dataset_config`[(here)](tao_project/spec_files/resnet18/dataset_config.txt) section of the training configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107a8a2-ffd6-4bc3-9a89-1062afdd53f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19bc4ae2-6f1d-4204-aecb-7316a1b33ec7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "dataset_config {\n",
    "  dataset: \"custom\"\n",
    "  augment: True\n",
    "  augmentation_config {\n",
    "    spatial_augmentation {\n",
    "      hflip_probability : 0.5\n",
    "      vflip_probability : 0.5\n",
    "      crop_and_resize_prob : 0.5\n",
    "    }\n",
    "  }\n",
    "  input_image_type: \"color\"\n",
    "  train_images_path:\"/workspace/tao-experiments/data/images/train\"\n",
    "  train_masks_path:\"/workspace/tao-experiments/data/masks/train\"\n",
    "\n",
    "  val_images_path:\"/workspace/tao-experiments/data/images/val\"\n",
    "  val_masks_path:\"/workspace/tao-experiments/data/masks/val\"\n",
    "  \n",
    "  test_images_path:\"/workspace/tao-experiments/data/images/val\"\n",
    "  \n",
    "  data_class_config {\n",
    "    target_classes {\n",
    "      name: \"notflood\"\n",
    "      mapping_class: \"notflood\"\n",
    "      label_id: 0\n",
    "    }\n",
    "    target_classes {\n",
    "      name: \"flood\"\n",
    "      mapping_class: \"flood\"\n",
    "      label_id: 255\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f334dfe-04fc-455a-a94d-9624cfb85c1a",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9eb38b-7041-4078-b4ad-60ea7086e2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2-e3'></a>\n",
    "### Exercise #3 - Modify Model Config ###\n",
    "The segmentation model can be configured using the `model_config` option in the spec file. \n",
    "* `all_projections (bool)`: For templates with shortcut connections, this parameter defines whether all shortcuts should be instantiated with 1x1 projection layers, irrespective of a change in stride across the input and output. \n",
    "* `arch (str)`: The architecture of the backbone feature extractor to be used for training. \n",
    "* `num_layers (int)`: The depth of the feature extractor for scalable templates. \n",
    "* `use_batch_norm (bool)`: A Boolean value that determines whether to use batch normalization layers or not. \n",
    "* `training_precision (dict)`: Contains a nested parameter that sets the precision of the back-end training framework. \n",
    "    * `backend_floatx`: The back-end training framework should be set to `FLOAT322`. \n",
    "* `initializer (choice)`: Initialization of convolutional layers. Supported initializations are `HE_UNIFORM`, `HE_NORMAL`, and `GLOROT_UNIFORM`. \n",
    "* `model_input_height (int)`: The model input height dimension of the model, which should be a multiple of 16.\n",
    "* `model_input_width (int)`: The model input width dimension of the model, which should be a multiple of 16.\n",
    "* `model_input_channels (int)`: The model-input channels dimension of the model, which should be set to 3 for a Resnet/VGG backbone. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `model_config`[(here)](tao_project/spec_files/resnet18/model_config.txt) section of the configuration file by changing the `<FIXME>`s into acceptable values and **save changes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac8ebc-0ede-4d6f-b982-47f6bf649b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/model_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eead900d-6d94-4a06-8b9c-c87d4bb99948",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "model_config {\n",
    "  model_input_width: 512\n",
    "  model_input_height: 512\n",
    "  model_input_channels: 3\n",
    "  num_layers: 18\n",
    "  all_projections: True\n",
    "  arch: \"resnet\"\n",
    "  use_batch_norm: False\n",
    "  training_precision {\n",
    "    backend_floatx: FLOAT32\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7fc6a-fb9b-4238-8b33-f5621a0b650c",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc06f5-3044-43b6-a35d-f7338e21ea3d",
   "metadata": {},
   "source": [
    "<a name='s2-e4'></a>\n",
    "#### Exercise #4 - Modify Training Config ####\n",
    "The `training_config` describes the training and learning process. \n",
    "* `batch_size (int)`: The number of images per batch per gpu. \n",
    "* `epochs (int)`: The number of epochs to train the model. One epoch represents one iteration of training through the entire dataset. \n",
    "* `log_summary_steps (int)`: The summary-steps interval at which train details are printed to stdout. \n",
    "* `checkpoint_interval (int)`: The number of epochs interval at which the checkpoint is saved. \n",
    "* `loss (str)`: The loss to be used for segmentation. \n",
    "* `learning_rate (float)`: The learning-rate initialization value. \n",
    "* `regularizer (dict)`: This parameter configures the type and weight of the regularizer to be used during training. The two parameters include:\n",
    "    * `type (Choice)`: The type of the regularizer being used should be `L2` or `L1`. \n",
    "    * `weight (Float)`: The floating-point weight of the regularizer. \n",
    "* `optimizer (dict)`: This parameter defines which optimizer to use for training, and the parameters to configure it, namely:\n",
    "    * `adam`: \n",
    "        * `epsilon (float)`: Is a very small number to prevent any division by zero in the implementation. \n",
    "        * `beta1 (float)`. \n",
    "        * `beta2 (float)`.\n",
    "* `activation (str)`: The activation to be used on the last layer supported is `softmax`. \n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `training_config`[(here)](tao_project/spec_files/resnet18/training_config.txt) section of the training configuration file by changing the `<FIXME>` into acceptable values and **save changes**. Pick a low (recommend 10) `num_epochs` to start as the higher the number of epochs the longer the model trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fff69-46a3-46a3-ad07-cf7947238405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# read the config file\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/training_config.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcdeb7d9-6d03-4614-8fec-c7e3dbc15988",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "training_config {\n",
    "  batch_size: 1\n",
    "  epochs: 30\n",
    "  log_summary_steps: 10\n",
    "  checkpoint_interval: 10\n",
    "  loss: \"cross_dice_sum\"\n",
    "  learning_rate: 0.0001\n",
    "  regularizer {\n",
    "    type: L2\n",
    "    weight: 2e-5\n",
    "  }\n",
    "  optimizer {\n",
    "    adam {\n",
    "      epsilon: 9.99999993923e-09\n",
    "      beta1: 0.899999976158\n",
    "      beta2: 0.999000012875\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35245c3-e8e1-4d16-a97b-c2cd97325f00",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac10ab-9555-42fa-bf53-8a4c81daaedc",
   "metadata": {},
   "source": [
    "<a name='s2-2.6'></a>\n",
    "### Combine Configuration Files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af8d63-2748-47b9-9bc1-ac561a34d6af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# combining configuration components in separate files and writing into one\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/dataset_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/model_config.txt \\\n",
    "     $LOCAL_SPECS_DIR/resnet18/training_config.txt \\\n",
    "     > $LOCAL_SPECS_DIR/resnet18/combined_config.txt\n",
    "!cat $LOCAL_SPECS_DIR/resnet18/combined_config.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd57b-41cf-432e-b2f3-033140bb8799",
   "metadata": {},
   "source": [
    "<a name='s2-2.7'></a>\n",
    "### Initiate Model Training ###\n",
    "After preparing input data and setting up a spec file. You are now ready to start training a semantic segmentation network."
   ]
  },
  {
   "cell_type": "raw",
   "id": "71e40905-6482-48dc-b81b-a5a5a9657f99",
   "metadata": {},
   "source": [
    "tao unet train [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                    -r <RESULTS_DIR>\n",
    "                    -n <MODEL_NAME>\n",
    "                    -m <PRETRAINED_MODEL_FILE>\n",
    "                    -k <key>\n",
    "                    [-v Set Verbosity of the logger]\n",
    "                    [--gpus GPUS]\n",
    "                    [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe0274-1990-48cf-b3d1-191d9e191f67",
   "metadata": {},
   "source": [
    "When using the `train` subtask, the `-e` argument indicates the path to the spec file, the `-r` argument indicates the result directory, and the `-k` indicates the key to _load_ the pre-trained weights. There are some optional arguments that might be useful such as `-n` to indicates the name of the final step model saved and `-m` to indicate the path to a pre-trained model to initialize. \n",
    "\n",
    "_Multi-GPU support can be enabled for those with the hardware using the `--gpus` argument. When running the training with more than one GPU, we will need to modify the `batch_size` and `learning_rate`. In most cases, scaling down the batch-size by a factor of NUM_GPU's or scaling up the learning rate by a factor of NUM_GPUs would be a good place to start._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890df497-2679-4f97-a175-7d76ad520d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous training if exists\n",
    "!rm -rf $LOCAL_EXPERIMENT_DIR/resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8ba85-1a06-424e-a4ec-6461de7df0d2",
   "metadata": {},
   "source": [
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "There are some `tensorflow:Entity` and \"deprecation\" warning messages that can be can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23671788-5215-4ab0-b6b4-09d9ab6738cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# train model\n",
    "!tao unet train -e $TAO_SPECS_DIR/resnet18/combined_config.txt \\\n",
    "                -r $TAO_EXPERIMENT_DIR/resnet18 \\\n",
    "                -n resnet18 \\\n",
    "                -m $TAO_EXPERIMENT_DIR/pretrained_resnet18/pretrained_semantic_segmentation_vresnet18/resnet_18.hdf5 \\\n",
    "                -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad44218-d756-4e76-883f-0fcc76044254",
   "metadata": {},
   "source": [
    "**Note**: The training may take hours to complete. `unet` supports restarting from checkpoints in case the training job is killed prematurely. Training from the closest checkpoint may be resumed by simply re-running the **same** command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92b108-f0f9-419b-b829-c05f7542c5e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "print('Model for every epoch at checkpoint_interval mentioned in the spec file:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/resnet18/\n",
    "!ls -ltrh $LOCAL_EXPERIMENT_DIR/resnet18/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc293375-1728-49ef-a331-a7d949ee6229",
   "metadata": {},
   "source": [
    "<a name='s2-2.8'></a>\n",
    "### Evaluating the Model ###\n",
    "The model should be evaluated for its performance at the end of training. The last step model saved in the `$USER_EXPERIMENT_DIR/resnet18_segmentation_long_epoch/weights` directory is used for evaluation, inference, and export. The evaluation metrics include precision, recall, f1-score, and IOU for every class. The evaluation also creates `results_tlt.json` as a record. We can evaluate the model with the `evaluate` subtask. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "fefc75a1-ce72-4f4f-9f78-ef8035bbbe03",
   "metadata": {},
   "source": [
    "tao unet evaluate [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                       -m <MODEL_PATH>\n",
    "                       -o <OUTPUT_DIR>\n",
    "                       -k <KEY>\n",
    "                       [--gpus GPUS]\n",
    "                       [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9d887-c398-4a6d-8c5a-e2e5629782a7",
   "metadata": {},
   "source": [
    "The `evaluate` subtask runs evaluation on the same validation set that was used during training. We can also run evaluation on an earlier model by editing the spec file to point to the intended model. When using the `evaluate` subtask, the `-e` argument indicates the path to the spec file, the `-m` argument indicates the path to the model, the `-o` argument indicates where the evaluation metrics outputs should be written, and the `-k` argument indicates the key to _load_ the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb363405-9ef2-4358-ac31-d80c692c2c8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# evaluate the model using the same validation set as training\n",
    "!tao unet evaluate -e $TAO_SPECS_DIR/resnet18/combined_config.txt\\\n",
    "                   -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                   -o $TAO_EXPERIMENT_DIR/resnet18/ \\\n",
    "                   -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0319a-b49c-4dd8-94b2-795c30bed000",
   "metadata": {},
   "source": [
    "<a name='s2-2.9'></a>\n",
    "### Visualizing Model Inference ###\n",
    "The `inference` subtask for `unet` may be used to visualize segmentation and generate frame-by-frame PNG format labels on a directory of images. The tool automatically generates segmentation overlayed images in `vis_overlay_tlt`. The labels will be generated in `mask_labels_tlt`. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d710380-3444-4947-ab09-e84711b94ed2",
   "metadata": {},
   "source": [
    "tao unet inference [-h] -e <EXPERIMENT_SPEC_FILE>\n",
    "                        -m <MODEL_PATH>\n",
    "                        -o <OUTPUT_DIR>\n",
    "                        -k <KEY>\n",
    "                        [--gpus GPUS]\n",
    "                        [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7201565-fe89-4335-ae26-2e8c3f765950",
   "metadata": {},
   "source": [
    "When using the `inference` subtask, the `-e` argument indicates the path to the inference spec file, the `-m` argument indicates the path to the model file, the `-o` argument indicates the path to the output images directory, and the `-k` argument indicates the key to _load_ the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38936aa-7709-405f-b655-6059f90670a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous inference\n",
    "!rm -rf $LOCAL_PROJECT_DIR/tao_infer_testing/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623818b-9146-482a-97eb-1b76458c2f38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# perform inference on the validation set\n",
    "!tao unet inference -e $TAO_SPECS_DIR/resnet18/combined_config.txt \\\n",
    "                    -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                    -o $TAO_PROJECT_DIR/tao_infer_testing \\\n",
    "                    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47fee3-fb55-4f1b-895d-9a2343061ffc",
   "metadata": {},
   "source": [
    "We can write a quick function that will help us sample random inferences. Execute the below cells to visualize the inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f4432-7f6b-4f08-be88-5c7eee82e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# define simple grid visualizer\n",
    "def visualize_images(num_images=10):\n",
    "    overlay_path=os.path.join(os.environ['LOCAL_PROJECT_DIR'], 'tao_infer_testing', 'vis_overlay_tlt')\n",
    "    inference_path=os.path.join(os.environ['LOCAL_PROJECT_DIR'], 'tao_infer_testing', 'mask_labels_tlt')\n",
    "    actual_path=os.path.join(os.environ['LOCAL_DATA_DIR'], 'masks', 'val')\n",
    "    inference_images_path=os.path.join(os.environ['LOCAL_DATA_DIR'], 'images', 'val')\n",
    "        \n",
    "    fig_dim=4\n",
    "    fig, ax_arr=plt.subplots(num_images, 4, figsize=[4*fig_dim, num_images*fig_dim], sharex=True, sharey=True)\n",
    "    ax_arr[0, 0].set_title('Overlay')\n",
    "    ax_arr[0, 1].set_title('Input')\n",
    "    ax_arr[0, 2].set_title('Inference')\n",
    "    ax_arr[0, 3].set_title('Actual')\n",
    "    ax_arr[0, 0].set_xticks([])\n",
    "    ax_arr[0, 0].set_yticks([])\n",
    "    \n",
    "    for idx, img_name in enumerate(random.sample(os.listdir(actual_path), num_images)):\n",
    "        ax_arr[idx, 0].imshow(plt.imread(overlay_path+'/'+img_name))\n",
    "        ax_arr[idx, 0].set_ylabel(img_name)\n",
    "        ax_arr[idx, 1].imshow(plt.imread(inference_images_path+'/'+img_name))\n",
    "        ax_arr[idx, 2].imshow(plt.imread(inference_path+'/'+img_name), cmap='gray')\n",
    "        ax_arr[idx, 3].imshow(plt.imread(actual_path+'/'+img_name), cmap='gray')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340185d-c1c8-4051-8998-222f1b9b102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# visualizing random images\n",
    "NUM_IMAGES = 4\n",
    "\n",
    "visualize_images(NUM_IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fdbad0-41a0-4fd7-b09f-6d8dabf5c9fc",
   "metadata": {},
   "source": [
    "<a name='s2-3'></a>\n",
    "## Model Export ##\n",
    "Once we are satisfied with our model, we can move to deployment. `unet` includes an `export` subtask to export and prepare a trained U-Net model for deployment. Exporting the model decouples the training process from deployment and allows conversion to TensorRT engines outside the TAO environment. TensorRT engines are specific to each hardware configuration and should be generated for each unique inference environment. This may be interchangeably referred to as the `.trt` or `.engine` file. The same exported TAO model may be used universally across training and deployment hardware. This is referred to as the `.etlt` file, or encrypted TAO file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8639a37-72d6-4e0f-95f4-d3de568f03a0",
   "metadata": {},
   "source": [
    "<a name='s2-3.1'></a>\n",
    "### TensorRT - Programmable Inference Accelerator\n",
    "\n",
    "NVIDIA [TensorRT](https://developer.nvidia.com/tensorrt) is a platform for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. TensorRT-based applications perform up to 40x faster than CPU-only platforms during inference. \n",
    "\n",
    "With TensorRT, you can optimize neural network models trained in all major frameworks, calibrate for lower precision with high accuracy, and finally deploy to hyperscale data centers, embedded, or automotive product platforms.\n",
    "\n",
    "Here are some great resources to learn more about TensorRT:\n",
    " \n",
    "* Main Page: https://developer.nvidia.com/tensorrt\n",
    "* Blogs: https://devblogs.nvidia.com/speed-up-inference-tensorrt/\n",
    "* Download: https://developer.nvidia.com/nvidia-tensorrt-download\n",
    "* Documentation: https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html\n",
    "* Sample Code: https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html\n",
    "* GitHub: https://github.com/NVIDIA/TensorRT\n",
    "* NGC Container: https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cd365-e7fd-4c8c-b03e-9d8c310b0e08",
   "metadata": {},
   "source": [
    "<a name='s2-3.2'></a>\n",
    "### Export the Trained Model ###\n",
    "When using the `export` subtask, the `-m` argument indicates the path to the `.tlt` model file to be exported, the `-e` argument indicates the path to the spec file, and `-k` argument indicates the key to _load_ the model. There are two optional arguments, `--gen_ds_config` and `--engine_file` that are useful for us. The `--gen_ds_config` argument indicates whether to generate a template inference configuration file as well as a label file. The `--engine_file` indicates the path to the serialized TensorRT engine file. \n",
    "<p><img src='images/important.png' width=720></p>\n",
    "\n",
    "Note that the TensorRT file is hardware specific and cannot be generalized across GPUs. Since a TensorRT engine file is hardware specific, you cannot use an engine file for deployment unless the deployment GPU is identical to the training GPU. This is true in our case since the Triton Inference Server will be deployed from the same hardware. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "afb18b3b-86a5-4c4f-9296-a310516a8c8f",
   "metadata": {},
   "source": [
    "tao unet export [-h] -e <EXPERIMENT_SPEC>\n",
    "                     -m <MODEL>\n",
    "                     -k <KEY>\n",
    "                     [--engine_file ENGINE_FILE]\n",
    "                     [--gen_ds_config]\n",
    "                     [--gpus GPUS]\n",
    "                     [--gpu_index GPU_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcae38a-6f81-4818-b428-bd653e208680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# remove any previous exports if exists\n",
    "!mkdir -p $LOCAL_EXPERIMENT_DIR/export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7c99c-2576-499e-8b37-347a1c20d4a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# export model and TensorRT engine\n",
    "!tao unet export -m $TAO_EXPERIMENT_DIR/resnet18/weights/resnet18.tlt \\\n",
    "                 -e $TAO_SPECS_DIR/resnet18/resnet18_config.txt \\\n",
    "                 -k $KEY \\\n",
    "                 --engine_file $TAO_EXPERIMENT_DIR/export/resnet18.engine \\\n",
    "                 --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01280ca6-e986-4fe6-9ec5-366e8cf6df8a",
   "metadata": {},
   "source": [
    "**Well Done!** Let's move to the [next notebook](./03_model_deployment_for_inference.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e715988-e892-4565-bcb9-85c289eefd2d",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
